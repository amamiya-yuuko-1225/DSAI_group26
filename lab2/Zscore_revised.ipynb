{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fb185bd-6638-4075-a62a-a63a7857d632",
   "metadata": {},
   "source": [
    "IDSAI_2024_lecture3_DEMO2 -------------------------------- 01000110.01001010 ---- revised: Aug2024_F.Jalalypour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02e78a5-a936-4e55-90fb-7b7bb563fc57",
   "metadata": {},
   "source": [
    "# Feature scaling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "87edd783-fdf3-4208-84d3-e12a3ca5ce1d",
   "metadata": {},
   "source": [
    "# Example 1\n",
    "Different feature scaling methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b4f168-e56c-44e7-b6e5-381bf26372c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Generating input data\n",
    "np.random.seed(0)  # For reproducibility\n",
    "feature_1 = np.random.randint(-200, 200, size=20)\n",
    "feature_2 = np.random.randint(-100, 150, size=20)\n",
    "\n",
    "# Absolute Maximum Scaling\n",
    "feature_1_abs_max = feature_1 / np.max(np.abs(feature_1))\n",
    "feature_2_abs_max = feature_2 / np.max(np.abs(feature_2))\n",
    "\n",
    "# Min-Max Scaling (Normalization)\n",
    "feature_1_min_max = (feature_1 - np.min(feature_1)) / (np.max(feature_1) - np.min(feature_1))\n",
    "feature_2_min_max = (feature_2 - np.min(feature_2)) / (np.max(feature_2) - np.min(feature_2))\n",
    "\n",
    "# Standardization (Z-score Normalization)\n",
    "feature_1_mean, feature_2_mean = np.mean(feature_1), np.mean(feature_2)\n",
    "feature_1_std, feature_2_std = np.std(feature_1), np.std(feature_2)\n",
    "\n",
    "feature_1_zscore = (feature_1 - feature_1_mean) / feature_1_std\n",
    "feature_2_zscore = (feature_2 - feature_2_mean) / feature_2_std\n",
    "\n",
    "# Plotting\n",
    "fig, axs = plt.subplots(2, 2, figsize=(14, 14))\n",
    "\n",
    "# Original Data\n",
    "axs[0, 0].scatter(feature_1, feature_2, color='blue')\n",
    "axs[0, 0].set_title('Original Data')\n",
    "axs[0, 0].set_xlabel('Feature 1')\n",
    "axs[0, 0].set_ylabel('Feature 2')\n",
    "axs[0, 0].grid(True)\n",
    "axs[0, 0].axhline(0, color='black', lw=1.5)\n",
    "axs[0, 0].axvline(0, color='black', lw=1.5)\n",
    "\n",
    "# Absolute Maximum Scaling\n",
    "axs[0, 1].scatter(feature_1_abs_max, feature_2_abs_max, color='green')\n",
    "axs[0, 1].set_title('Absolute Maximum Scaling')\n",
    "axs[0, 1].set_xlabel('Scaled Feature 1')\n",
    "axs[0, 1].set_ylabel('Scaled Feature 2')\n",
    "axs[0, 1].grid(True)\n",
    "axs[0, 1].axhline(0, color='black', lw=1.5)\n",
    "axs[0, 1].axvline(0, color='black', lw=1.5)\n",
    "\n",
    "# Min-Max Scaling\n",
    "axs[1, 0].scatter(feature_1_min_max, feature_2_min_max, color='orange')\n",
    "axs[1, 0].set_title('Min-Max Scaling (Normalization)')\n",
    "axs[1, 0].set_xlabel('Scaled Feature 1')\n",
    "axs[1, 0].set_ylabel('Scaled Feature 2')\n",
    "axs[1, 0].grid(True)\n",
    "axs[1, 0].axhline(0, color='black', lw=1.5)\n",
    "axs[1, 0].axvline(0, color='black', lw=1.5)\n",
    "\n",
    "# Standardization (Z-score Normalization)\n",
    "axs[1, 1].scatter(feature_1_zscore, feature_2_zscore, color='red')\n",
    "axs[1, 1].set_title('Standardization (Z-score Normalization)')\n",
    "axs[1, 1].set_xlabel('Scaled Feature 1')\n",
    "axs[1, 1].set_ylabel('Scaled Feature 2')\n",
    "axs[1, 1].grid(True)\n",
    "axs[1, 1].axhline(0, color='black', lw=1.5)\n",
    "axs[1, 1].axvline(0, color='black', lw=1.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695b12ef-da3a-4e01-8c7d-71e0ab0e97a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming feature_1_zscore is already calculated as shown previously\n",
    "# Calculate the mean and standard deviation of feature_1_zscore\n",
    "mean_feature_1_zscore = np.mean(feature_1_zscore)\n",
    "std_feature_1_zscore = np.std(feature_1_zscore)\n",
    "\n",
    "print(f\"Mean of feature_1_zscore: {mean_feature_1_zscore}\")\n",
    "print(f\"Standard Deviation of feature_1_zscore: {std_feature_1_zscore}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c6dc20-1538-40cf-a262-fdee56cf5a49",
   "metadata": {},
   "source": [
    "# Example 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110732b2-1704-49ba-948c-75c5469d699e",
   "metadata": {},
   "source": [
    "## Z- score calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd0a50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('penguins_size.csv')\n",
    "# Drop any rows with missing values\n",
    "df_clear = df.dropna()\n",
    "#Define the numeric columns we want to focus on\n",
    "numeric_columns = ['culmen_length_mm','culmen_depth_mm','flipper_length_mm','body_mass_g']\n",
    "#  Display the extracted columns\n",
    "df_clear[numeric_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b0fa7e",
   "metadata": {},
   "source": [
    "We can compute the z-score manually by computing the mean and the standard deviation. `ddof=1` applies Bessel's corrrection, although it is often unnecessary if the number of observations is large."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666e9c9e-8e10-4142-a105-99d2454ca282",
   "metadata": {},
   "source": [
    "# Calculate mean for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378e50f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean=df_clear[numeric_columns].mean()\n",
    "mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8646149-f894-43de-981a-7a56a1ca203b",
   "metadata": {},
   "source": [
    "# Calculate sample variance for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e4afe0-dc79-495c-9418-7c24b53a4971",
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_values = df_clear[numeric_columns].var()\n",
    "variance_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986ef986-4870-4117-8664-bee49a7e0a14",
   "metadata": {},
   "source": [
    "# Calculate standard deviation for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26e05c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SD=df_clear[numeric_columns].std(ddof=1)\n",
    "SD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fde3f36-e575-4fa7-bdcb-a98c2c4a8aaa",
   "metadata": {},
   "source": [
    "# Calculate z-scores for each value in the numeric columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de7728b-369d-45ec-a0c1-43d403bf888b",
   "metadata": {},
   "source": [
    "### manually calculating z-scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8600d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_scores_manual = (df_clear[numeric_columns] - df_clear[numeric_columns].mean()) / df_clear[numeric_columns].std(ddof=1)\n",
    "print(z_scores_manual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfbdb89",
   "metadata": {},
   "source": [
    "SciPy comes with `scipy.stats.zscore` that does the same. By default, it does not apply Bessel's correction, but we can enable that with `ddof=1`. We can apply the function to the entire column with `apply`. If the dataframe contained nonnumerical columns, we might want to choose those columns first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ba6a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "z_scores = df_clear[numeric_columns].apply(zscore,ddof=1)\n",
    "z_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5856bd55-e5ea-42dc-840f-f99853f214d6",
   "metadata": {},
   "source": [
    "### calculate z-scores by Scikit-learn "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc472c10",
   "metadata": {},
   "source": [
    "Scikit-learn comes with `StandardScaler` that also does the same, but Bessel's correction is not used. This is inconsequential if *n* is large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555f5e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ss = StandardScaler()\n",
    "ss.fit_transform(df_clear[numeric_columns].to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c2b857",
   "metadata": {},
   "source": [
    "Standard scaler is particularly useful if we need to apply the same transformation to other datapoints (e.g., a separate test set). We want to use the same scale we determined with the training set. Calling `fit` or `fit_transform` will store the mean and standard deviation in the model. Future calls to `transform` (*without* `fit`) will then use the same values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8529c341-46f7-454d-aa95-4f9b02e64f2b",
   "metadata": {},
   "source": [
    "Thank you for your attention :) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
