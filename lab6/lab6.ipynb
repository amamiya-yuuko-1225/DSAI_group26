{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAT565/DIT407 Assignment 6\n",
    "\n",
    "Author: Group 26 | Wenjun Tian wenjunt@chalmers.se | Yifan Tang yifant@chalmers.se\n",
    "\n",
    "Date: 2024-12-14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: The dataset\n",
    "We load both training and test set using the `torchvision.datasets`\n",
    "module, and then use the `transform` option to convert the images from `PIL` into `tensors` (and normalize the grey values to $[0,1]$)\n",
    "\n",
    "The images from both sets and the verification of their dimensions and grey value normaliztion are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifiy image dimensions and value normalization:\n",
      "Image size: 28 x 28\n",
      "Min pixel grey value: 0.0\n",
      "Max pixel grey value: 1.0\n",
      "\n",
      "\n",
      "Here are 4 imags from the trainning set (line 1) and 4 images from the test set (line2)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x250 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x250 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Download training data from open datasets.\n",
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    #convert PIL image to Tensors; normalize grey values to [0,1]\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "# Get data iterators from loaders\n",
    "train_dataiter = iter(train_dataloader)\n",
    "train_images, train_labels = next(train_dataiter)\n",
    "test_dataiter = iter(test_dataloader)\n",
    "test_images, test_labels = next(test_dataiter)\n",
    "\n",
    "print(\"Verifiy image dimensions and value normalization:\")\n",
    "# Verify image dimensions\n",
    "print(f\"Image size: {train_images.shape[2]} x {train_images.shape[3]}\")\n",
    "# Verify grey values normaliztion\n",
    "print(f\"Min pixel grey value: {train_images.min()}\")\n",
    "print(f\"Max pixel grey value: {train_images.max()}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# Use matplotlib to plot 4 images from each set in greyscale\n",
    "\n",
    "print(\"Here are 4 imags from the trainning set (line 1) and 4 images from the test set (line2)\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(10, 2.5))\n",
    "for i in range(4):\n",
    "    image = train_images[i].numpy().squeeze()\n",
    "    axes[i].imshow(image, cmap='gray')\n",
    "    axes[i].set_title(f'Label: {train_labels[i].item()}')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(10, 2.5))\n",
    "for i in range(4):\n",
    "    image = test_images[i].numpy().squeeze()\n",
    "    axes[i].imshow(image, cmap='gray')\n",
    "    axes[i].set_title(f'Label: {test_labels[i].item()}')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Single hidden layer\n",
    "\n",
    "## 2.1 Relevant Parameters:\n",
    "1. Network Architecture: One hidden layer with 128 neurons (suggestion from Google).\n",
    "2. Activation Function: ReLU.\n",
    "3. Loss Function: Cross Entropy Loss.\n",
    "4. Optimizer: Stochastic Gradient Descent (SGD) with a learning rate of 0.01.\n",
    "5. Batch Size: 64 (defined in the previous code cell).\n",
    "6. Number of Epochs: 10.\n",
    "\n",
    "## 2.2 Validation accuracy of the network\n",
    "\n",
    "Results are shown below the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy of the network (with respect to the test set) after each epoch:\n",
      "Epoch [1/10], Validation Accuracy: 86.12%\n",
      "Epoch [2/10], Validation Accuracy: 89.25%\n",
      "Epoch [3/10], Validation Accuracy: 90.30%\n",
      "Epoch [4/10], Validation Accuracy: 90.97%\n",
      "Epoch [5/10], Validation Accuracy: 91.57%\n",
      "Epoch [6/10], Validation Accuracy: 92.03%\n",
      "Epoch [7/10], Validation Accuracy: 92.25%\n",
      "Epoch [8/10], Validation Accuracy: 92.61%\n",
      "Epoch [9/10], Validation Accuracy: 92.92%\n",
      "Epoch [10/10], Validation Accuracy: 93.14%\n"
     ]
    }
   ],
   "source": [
    "class FeedforwardNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeedforwardNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 128)  # Input layer to hidden layer\n",
    "        self.relu = nn.ReLU()             # ReLU activation function\n",
    "        self.fc2 = nn.Linear(128, 10)     # Hidden layer to output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)  # Flatten the input from a 28x28 matrix to a (28^2)x1 vector\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = FeedforwardNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "num_epochs = 10\n",
    "\n",
    "def train_and_validate(model, criterion, optimizer, num_epochs):\n",
    "    print(\"Validation accuracy of the network (with respect to the test set) after each epoch:\")\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        for images, labels in train_dataloader:\n",
    "            # Set previous grads to 0 before the new round of back propagation\n",
    "            optimizer.zero_grad()\n",
    "            # Get outputs\n",
    "            outputs = model(images)\n",
    "            # Preform back propagation, record grads\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            # Update weights/biases using the computed grads\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_dataloader:\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Validation Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "train_and_validate(model=model, criterion=criterion, optimizer=optimizer, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: Two hidden layers\n",
    "\n",
    "The code and results are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy of the network (with respect to the test set) after each epoch:\n",
      "Epoch [1/60], Validation Accuracy: 81.94%\n",
      "Epoch [2/60], Validation Accuracy: 88.48%\n",
      "Epoch [3/60], Validation Accuracy: 90.08%\n",
      "Epoch [4/60], Validation Accuracy: 91.23%\n",
      "Epoch [5/60], Validation Accuracy: 91.92%\n",
      "Epoch [6/60], Validation Accuracy: 92.54%\n",
      "Epoch [7/60], Validation Accuracy: 93.15%\n",
      "Epoch [8/60], Validation Accuracy: 93.48%\n",
      "Epoch [9/60], Validation Accuracy: 93.86%\n",
      "Epoch [10/60], Validation Accuracy: 94.08%\n",
      "Epoch [11/60], Validation Accuracy: 94.30%\n",
      "Epoch [12/60], Validation Accuracy: 94.63%\n",
      "Epoch [13/60], Validation Accuracy: 94.91%\n",
      "Epoch [14/60], Validation Accuracy: 95.12%\n",
      "Epoch [15/60], Validation Accuracy: 95.39%\n",
      "Epoch [16/60], Validation Accuracy: 95.50%\n",
      "Epoch [17/60], Validation Accuracy: 95.62%\n",
      "Epoch [18/60], Validation Accuracy: 95.83%\n",
      "Epoch [19/60], Validation Accuracy: 96.02%\n",
      "Epoch [20/60], Validation Accuracy: 96.19%\n",
      "Epoch [21/60], Validation Accuracy: 96.27%\n",
      "Epoch [22/60], Validation Accuracy: 96.39%\n",
      "Epoch [23/60], Validation Accuracy: 96.48%\n",
      "Epoch [24/60], Validation Accuracy: 96.59%\n",
      "Epoch [25/60], Validation Accuracy: 96.69%\n",
      "Epoch [26/60], Validation Accuracy: 96.79%\n",
      "Epoch [27/60], Validation Accuracy: 96.82%\n",
      "Epoch [28/60], Validation Accuracy: 96.91%\n",
      "Epoch [29/60], Validation Accuracy: 97.01%\n",
      "Epoch [30/60], Validation Accuracy: 97.02%\n",
      "Epoch [31/60], Validation Accuracy: 97.05%\n",
      "Epoch [32/60], Validation Accuracy: 97.14%\n",
      "Epoch [33/60], Validation Accuracy: 97.21%\n",
      "Epoch [34/60], Validation Accuracy: 97.24%\n",
      "Epoch [35/60], Validation Accuracy: 97.31%\n",
      "Epoch [36/60], Validation Accuracy: 97.36%\n",
      "Epoch [37/60], Validation Accuracy: 97.42%\n",
      "Epoch [38/60], Validation Accuracy: 97.49%\n",
      "Epoch [39/60], Validation Accuracy: 97.53%\n",
      "Epoch [40/60], Validation Accuracy: 97.56%\n",
      "Epoch [41/60], Validation Accuracy: 97.60%\n",
      "Epoch [42/60], Validation Accuracy: 97.64%\n",
      "Epoch [43/60], Validation Accuracy: 97.66%\n",
      "Epoch [44/60], Validation Accuracy: 97.69%\n",
      "Epoch [45/60], Validation Accuracy: 97.76%\n",
      "Epoch [46/60], Validation Accuracy: 97.76%\n",
      "Epoch [47/60], Validation Accuracy: 97.76%\n",
      "Epoch [48/60], Validation Accuracy: 97.79%\n",
      "Epoch [49/60], Validation Accuracy: 97.81%\n",
      "Epoch [50/60], Validation Accuracy: 97.81%\n",
      "Epoch [51/60], Validation Accuracy: 97.83%\n",
      "Epoch [52/60], Validation Accuracy: 97.86%\n",
      "Epoch [53/60], Validation Accuracy: 97.86%\n",
      "Epoch [54/60], Validation Accuracy: 97.86%\n",
      "Epoch [55/60], Validation Accuracy: 97.88%\n",
      "Epoch [56/60], Validation Accuracy: 97.90%\n",
      "Epoch [57/60], Validation Accuracy: 97.90%\n",
      "Epoch [58/60], Validation Accuracy: 97.90%\n",
      "Epoch [59/60], Validation Accuracy: 97.91%\n",
      "Epoch [60/60], Validation Accuracy: 97.92%\n"
     ]
    }
   ],
   "source": [
    "# Define the neural network\n",
    "class FeedforwardNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeedforwardNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 500) \n",
    "        self.fc2 = nn.Linear(500, 300)\n",
    "        self.fc3 = nn.Linear(300, 10)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Create the model, loss function, and optimizer\n",
    "model = FeedforwardNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=2e-4)\n",
    "num_epochs = 60\n",
    "\n",
    "train_and_validate(model=model, criterion=criterion, optimizer=optimizer, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4: Convolutional neural network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy of the network (with respect to the test set) after each epoch:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2e-4\u001b[39m) \n\u001b[1;32m     22\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m60\u001b[39m\n\u001b[0;32m---> 24\u001b[0m \u001b[43mtrain_and_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 42\u001b[0m, in \u001b[0;36mtrain_and_validate\u001b[0;34m(model, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m test_dataloader:\n\u001b[0;32m---> 42\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m         _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     44\u001b[0m         total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/lab1/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lab1/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[23], line 11\u001b[0m, in \u001b[0;36mConvNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 11\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     12\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)))\n\u001b[1;32m     13\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m64\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m7\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m7\u001b[39m)  \u001b[38;5;66;03m# Flatten the tensor\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 7 * 7)  # Flatten the tensor\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create the model, loss function, and optimizer\n",
    "model = ConvNet()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=2e-4) \n",
    "num_epochs = 60\n",
    "\n",
    "train_and_validate(model=model, criterion=criterion, optimizer=optimizer, num_epochs=num_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
